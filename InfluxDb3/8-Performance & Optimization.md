# 8ï¸âƒ£ Performance & Optimization

---

## 8.1 Performance Overview - Key Metrics

### **Performance Dimensions:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           InfluxDB Performance Metrics                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Write Performance:                                      â”‚
â”‚  â”œâ”€ Throughput:      10K-100K+ points/sec               â”‚
â”‚  â”œâ”€ Latency:         1-10ms (p99)                       â”‚
â”‚  â””â”€ Batch size:      5K-10K points optimal              â”‚
â”‚                                                          â”‚
â”‚  Query Performance:                                      â”‚
â”‚  â”œâ”€ Simple queries:  <50ms                              â”‚
â”‚  â”œâ”€ Aggregations:    50-500ms                           â”‚
â”‚  â”œâ”€ Complex joins:   500ms-5s                           â”‚
â”‚  â””â”€ Full scan:       5s+ (avoid!)                       â”‚
â”‚                                                          â”‚
â”‚  Storage Efficiency:                                     â”‚
â”‚  â”œâ”€ Compression:     3-10x (depending on data)          â”‚
â”‚  â”œâ”€ Index overhead:  5-15% of data size                 â”‚
â”‚  â””â”€ Compaction:      Reduces size by 50-90%             â”‚
â”‚                                                          â”‚
â”‚  Resource Usage:                                         â”‚
â”‚  â”œâ”€ CPU:            2-8 cores for moderate load         â”‚
â”‚  â”œâ”€ Memory:         4-32 GB recommended                 â”‚
â”‚  â”œâ”€ Disk I/O:       SSD recommended (10x faster)        â”‚
â”‚  â””â”€ Network:        1-10 Gbps for high throughput       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8.2 Write Performance Tuning

### **Optimization 1: Optimal Batch Size**

**Benchmark Test:**

python

```python
import time
import requests

INFLUX_URL = "http://localhost:8086/api/v2/write"
INFLUX_ORG = "my-org"
INFLUX_BUCKET = "benchmark"
INFLUX_TOKEN = "YOUR_TOKEN"

def benchmark_batch_size(batch_sizes, total_points=100000):
    """Benchmark different batch sizes"""
  
    results = {}
  
    for batch_size in batch_sizes:
        print(f"\nTesting batch size: {batch_size}")
  
        # Generate test data
        points = []
        for i in range(total_points):
            timestamp = int(time.time() * 1e9) + i * 1000000
            line = f"benchmark,host=server01 value={i} {timestamp}"
            points.append(line)
  
        # Write in batches
        start_time = time.time()
        batches_written = 0
  
        for i in range(0, total_points, batch_size):
            batch = points[i:i+batch_size]
            batch_data = "\n".join(batch)
  
            response = requests.post(
                f"{INFLUX_URL}?org={INFLUX_ORG}&bucket={INFLUX_BUCKET}",
                headers={
                    "Authorization": f"Token {INFLUX_TOKEN}",
                    "Content-Type": "text/plain"
                },
                data=batch_data
            )
  
            if response.status_code == 204:
                batches_written += 1
            else:
                print(f"Error: {response.status_code}")
  
        elapsed = time.time() - start_time
        throughput = total_points / elapsed
  
        results[batch_size] = {
            'elapsed_time': elapsed,
            'batches_written': batches_written,
            'points_per_second': throughput,
            'avg_batch_time': elapsed / batches_written
        }
  
        print(f"  Elapsed: {elapsed:.2f}s")
        print(f"  Throughput: {throughput:.0f} points/sec")
        print(f"  Avg batch time: {elapsed/batches_written*1000:.2f}ms")
  
    return results

# Test different batch sizes
batch_sizes = [100, 500, 1000, 5000, 10000, 20000]
results = benchmark_batch_size(batch_sizes)

# Print comparison
print("\n" + "="*60)
print("BATCH SIZE COMPARISON")
print("="*60)
print(f"{'Batch Size':<12} {'Time (s)':<12} {'Points/sec':<15} {'Batch Time (ms)'}")
print("-"*60)
for size, data in results.items():
    print(f"{size:<12} {data['elapsed_time']:<12.2f} "
          f"{data['points_per_second']:<15.0f} "
          f"{data['avg_batch_time']*1000:<.2f}")
```

**Expected Results:**

```
Batch Size   Time (s)     Points/sec      Batch Time (ms)
------------------------------------------------------------
100          25.50        3922            25.50
500          12.30        8130            12.30
1000         8.50         11765           8.50
5000         5.20         19231           10.40  â† Optimal
10000        5.00         20000           10.00  â† Optimal
20000        5.50         18182           11.00  (diminishing returns)
```

**Recommendation:**

```
âœ… Optimal batch size: 5,000 - 10,000 points
âœ… Sweet spot for throughput vs latency
âŒ <1,000: Too many network round trips
âŒ >20,000: Diminishing returns, higher memory usage
```

---

### **Optimization 2: Parallel Writes**

python

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import threading

class ParallelWriter:
    def __init__(self, influx_url, org, bucket, token, num_workers=4):
        self.influx_url = influx_url
        self.org = org
        self.bucket = bucket
        self.token = token
        self.num_workers = num_workers
  
        self.write_queue = queue.Queue(maxsize=1000)
        self.stats = {
            'points_written': 0,
            'batches_written': 0,
            'errors': 0
        }
        self.stats_lock = threading.Lock()
  
    def write_batch(self, batch):
        """Write single batch to InfluxDB"""
  
        data = "\n".join(batch)
  
        try:
            response = requests.post(
                f"{self.influx_url}/api/v2/write",
                params={'org': self.org, 'bucket': self.bucket},
                headers={
                    'Authorization': f'Token {self.token}',
                    'Content-Type': 'text/plain'
                },
                data=data,
                timeout=10
            )
  
            if response.status_code == 204:
                with self.stats_lock:
                    self.stats['points_written'] += len(batch)
                    self.stats['batches_written'] += 1
                return True
            else:
                with self.stats_lock:
                    self.stats['errors'] += 1
                return False
  
        except Exception as e:
            with self.stats_lock:
                self.stats['errors'] += 1
            print(f"Write error: {e}")
            return False
  
    def worker(self, worker_id):
        """Worker thread that processes write queue"""
  
        batch = []
        batch_size = 5000
  
        while True:
            try:
                # Get point from queue (blocking with timeout)
                point = self.write_queue.get(timeout=1)
  
                if point is None:  # Shutdown signal
                    if batch:
                        self.write_batch(batch)
                    break
  
                batch.append(point)
  
                # Write when batch is full
                if len(batch) >= batch_size:
                    self.write_batch(batch)
                    batch = []
  
            except queue.Empty:
                # Timeout - flush existing batch
                if batch:
                    self.write_batch(batch)
                    batch = []
  
    def start(self):
        """Start worker threads"""
  
        self.executor = ThreadPoolExecutor(max_workers=self.num_workers)
        self.futures = []
  
        for i in range(self.num_workers):
            future = self.executor.submit(self.worker, i)
            self.futures.append(future)
  
        print(f"Started {self.num_workers} worker threads")
  
    def add_point(self, line_protocol):
        """Add point to write queue (non-blocking)"""
        self.write_queue.put(line_protocol)
  
    def stop(self):
        """Stop all workers and wait for completion"""
  
        # Send shutdown signal to all workers
        for _ in range(self.num_workers):
            self.write_queue.put(None)
  
        # Wait for all workers to finish
        self.executor.shutdown(wait=True)
  
        print(f"\nFinal stats:")
        print(f"  Points written: {self.stats['points_written']:,}")
        print(f"  Batches written: {self.stats['batches_written']:,}")
        print(f"  Errors: {self.stats['errors']:,}")

# Usage
writer = ParallelWriter(
    influx_url="http://localhost:8086",
    org="my-org",
    bucket="my-bucket",
    token="YOUR_TOKEN",
    num_workers=4
)

writer.start()

# Add points (non-blocking)
start = time.time()
for i in range(100000):
    timestamp = int(time.time() * 1e9) + i * 1000000
    line = f"benchmark,host=server{i%10:02d} value={i} {timestamp}"
    writer.add_point(line)

# Wait for completion
writer.stop()
elapsed = time.time() - start

print(f"\nTotal time: {elapsed:.2f}s")
print(f"Throughput: {100000/elapsed:.0f} points/sec")
```

**Performance Comparison:**

```
Single-threaded:   20,000 points/sec
4 workers:         60,000 points/sec (3x faster)
8 workers:         90,000 points/sec (4.5x faster)
16 workers:        95,000 points/sec (diminishing returns)
```

---

### **Optimization 3: Connection Pooling**

python

```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class OptimizedInfluxClient:
    def __init__(self, url, org, token, pool_size=20):
        self.url = url
        self.org = org
        self.token = token
  
        # Create session with connection pooling
        self.session = requests.Session()
  
        # Configure retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
  
        # Configure adapter with connection pooling
        adapter = HTTPAdapter(
            pool_connections=pool_size,
            pool_maxsize=pool_size,
            max_retries=retry_strategy,
            pool_block=False
        )
  
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
  
        # Set default headers
        self.session.headers.update({
            'Authorization': f'Token {token}',
            'Content-Type': 'text/plain; charset=utf-8'
        })
  
    def write(self, bucket, data):
        """Write data using pooled connection"""
  
        response = self.session.post(
            f"{self.url}/api/v2/write",
            params={'org': self.org, 'bucket': bucket},
            data=data,
            timeout=10
        )
  
        return response.status_code == 204
  
    def close(self):
        """Close session and release connections"""
        self.session.close()

# Usage
client = OptimizedInfluxClient(
    url="http://localhost:8086",
    org="my-org",
    token="YOUR_TOKEN",
    pool_size=20
)

# Reuse connections across multiple writes
for batch in batches:
    client.write("my-bucket", batch)

client.close()
```

**Benefits:**

```
Without pooling:  New TCP connection per request
                  Overhead: ~5-10ms per request

With pooling:     Reuse existing connections
                  Overhead: <1ms per request

Result: 5-10x faster for high-frequency writes
```

---

### **Optimization 4: Compression**

python

```python
import gzip
import io

def write_compressed(data, compression_level=6):
    """Write gzip-compressed data"""
  
    # Compress data
    buffer = io.BytesIO()
    with gzip.GzipFile(fileobj=buffer, mode='wb', compresslevel=compression_level) as gz:
        gz.write(data.encode('utf-8'))
  
    compressed_data = buffer.getvalue()
  
    # Calculate compression ratio
    original_size = len(data.encode('utf-8'))
    compressed_size = len(compressed_data)
    ratio = original_size / compressed_size
  
    print(f"Original: {original_size:,} bytes")
    print(f"Compressed: {compressed_size:,} bytes")
    print(f"Ratio: {ratio:.2f}x")
  
    # Write with compression
    response = requests.post(
        f"{INFLUX_URL}/api/v2/write",
        params={'org': INFLUX_ORG, 'bucket': INFLUX_BUCKET},
        headers={
            'Authorization': f'Token {INFLUX_TOKEN}',
            'Content-Type': 'text/plain; charset=utf-8',
            'Content-Encoding': 'gzip'
        },
        data=compressed_data,
        timeout=10
    )
  
    return response.status_code == 204

# Test compression
large_batch = []
for i in range(10000):
    line = f"metrics,host=server{i%100:02d},region=us-west,env=prod cpu={70+i%30},memory={60+i%40},disk={50+i%50} {int(time.time()*1e9)}"
    large_batch.append(line)

data = "\n".join(large_batch)
write_compressed(data)
```

**Compression Results:**

```
Original:    2,450,000 bytes (2.45 MB)
Compressed:    350,000 bytes (0.35 MB)
Ratio:       7.0x

Network savings: ~85% bandwidth reduction
Transfer time:   ~7x faster on slow networks
```

---

## 8.3 Query Performance Optimization

### **Optimization 1: Time Range Filtering**

**âŒ Bad (Full table scan):**

sql

```sql
SELECT AVG(cpu_percent) 
FROM cpu_metrics
WHERE host = 'server01'
```

**Query plan:**

```
- Scan all data files
- Filter by host
- Calculate average
- Time: 5000ms+ (depending on data size)
```

---

**âœ… Good (Time-bounded):**

sql

```sql
SELECT AVG(cpu_percent)
FROM cpu_metrics
WHERE time >= now() - INTERVAL '1 hour'
  AND host = 'server01'
```

**Query plan:**

```
- Identify relevant files by time (predicate pushdown)
- Scan only recent files
- Filter by host
- Calculate average
- Time: 50ms (100x faster!)
```

---

### **Optimization 2: Column Projection**

**âŒ Bad (Read all columns):**

sql

```sql
SELECT *
FROM system_metrics
WHERE time >= now() - INTERVAL '1 hour'
```

**Data read:**

```
Columns: time, host, cpu, memory, disk, network_in, network_out, ...
Total:   ~100 MB read from disk
```

---

**âœ… Good (Read only needed columns):**

sql

```sql
SELECT time, host, cpu
FROM system_metrics
WHERE time >= now() - INTERVAL '1 hour'
```

**Data read:**

```
Columns: time, host, cpu
Total:   ~15 MB read from disk (7x less!)
```

---

### **Optimization 3: Aggregation Pushdown**

**âŒ Bad (Client-side aggregation):**

python

```python
# Query all data
results = query("""
    SELECT cpu_percent 
    FROM cpu_metrics 
    WHERE time >= now() - INTERVAL '7 days'
""")

# Aggregate in Python (100K+ rows transferred!)
avg_cpu = sum(r['cpu_percent'] for r in results) / len(results)
```

**Performance:**

```
- Data transferred: 100,000 rows Ã— 100 bytes = 10 MB
- Network time: 1000ms
- Processing time: 500ms
- Total: 1500ms
```

---

**âœ… Good (Server-side aggregation):**

python

```python
# Aggregate in database
results = query("""
    SELECT AVG(cpu_percent) as avg_cpu
    FROM cpu_metrics
    WHERE time >= now() - INTERVAL '7 days'
""")

avg_cpu = results[0]['avg_cpu']
```

**Performance:**

```
- Data transferred: 1 row Ã— 20 bytes = 20 bytes
- Network time: 5ms
- Processing time: 50ms
- Total: 55ms (27x faster!)
```

---

### **Optimization 4: Appropriate Time Windows**

python

```python
def query_with_adaptive_window(time_range_hours):
    """Use appropriate time window based on range"""
  
    if time_range_hours <= 1:
        # Real-time: 10-second buckets
        interval = "10 seconds"
    elif time_range_hours <= 24:
        # Last day: 5-minute buckets
        interval = "5 minutes"
    elif time_range_hours <= 168:  # 7 days
        # Last week: 1-hour buckets
        interval = "1 hour"
    else:
        # Long-term: 1-day buckets
        interval = "1 day"
  
    query = f"""
    SELECT 
        date_bin(INTERVAL '{interval}', time) as time_bucket,
        AVG(cpu_percent) as avg_cpu
    FROM cpu_metrics
    WHERE time >= now() - INTERVAL '{time_range_hours} hours'
    GROUP BY time_bucket
    ORDER BY time_bucket
    """
  
    return query

# Example usage
query_1h = query_with_adaptive_window(1)     # 360 rows (10s buckets)
query_24h = query_with_adaptive_window(24)   # 288 rows (5m buckets)
query_7d = query_with_adaptive_window(168)   # 168 rows (1h buckets)
```

**Result size comparison:**

```
Time Range   Raw Points   Window    Result Rows   Speedup
----------------------------------------------------------
1 hour       360,000      10s       360           1000x
24 hours     8,640,000    5min      288           30,000x
7 days       60,480,000   1hour     168           360,000x
```

---

### **Optimization 5: Index Usage**

**Understanding what's indexed:**

sql

```sql
-- âœ… FAST: Tags are indexed
SELECT * FROM metrics
WHERE host = 'server01'          -- Indexed (tag)
  AND environment = 'production' -- Indexed (tag)
  AND time >= now() - INTERVAL '1 hour'  -- Indexed

-- âŒ SLOW: Fields are not indexed
SELECT * FROM metrics
WHERE user_id = 'user_12345'     -- NOT indexed (field)
  AND time >= now() - INTERVAL '1 hour'
```

**Solution: Move high-cardinality identifiers to fields, create tag for filtering:**

sql

```sql
-- Schema design
measurement: api_requests
tags:
  - user_segment: 'premium', 'free', 'enterprise'  (low cardinality)
  - endpoint: '/api/users', '/api/products', ...   (moderate cardinality)
fields:
  - user_id: 'user_12345'                         (high cardinality)
  - request_id: 'req_abc123'                      (very high cardinality)

-- Query
SELECT * FROM api_requests
WHERE user_segment = 'premium'   -- Fast (indexed tag)
  AND endpoint = '/api/users'    -- Fast (indexed tag)
  AND time >= now() - INTERVAL '1 hour'
```

---

## 8.4 Query Performance Monitoring

### **Query Profiling Script:**

python

```python
import time
import requests
import json

class QueryProfiler:
    def __init__(self, influx_url, org, token):
        self.influx_url = influx_url
        self.org = org
        self.token = token
  
    def profile_query(self, query, runs=5):
        """Profile query performance"""
  
        print(f"\nProfiling query (runs={runs}):")
        print(f"SQL: {query[:100]}...")
  
        timings = []
  
        for i in range(runs):
            start = time.time()
  
            response = requests.post(
                f"{self.influx_url}/api/v2/query",
                params={'org': self.org},
                headers={
                    'Authorization': f'Token {self.token}',
                    'Content-Type': 'application/json'
                },
                json={'query': query, 'type': 'sql'},
                timeout=30
            )
  
            elapsed = time.time() - start
            timings.append(elapsed)
  
            if response.status_code == 200:
                result_size = len(response.content)
                row_count = len(response.json().get('data', []))
            else:
                print(f"Error: {response.status_code}")
                return None
  
        # Calculate statistics
        avg_time = sum(timings) / len(timings)
        min_time = min(timings)
        max_time = max(timings)
  
        print(f"\nResults:")
        print(f"  Runs:         {runs}")
        print(f"  Avg time:     {avg_time*1000:.2f}ms")
        print(f"  Min time:     {min_time*1000:.2f}ms")
        print(f"  Max time:     {max_time*1000:.2f}ms")
        print(f"  Result size:  {result_size:,} bytes")
        print(f"  Row count:    {row_count:,} rows")
        print(f"  Throughput:   {row_count/avg_time:.0f} rows/sec")
  
        return {
            'avg_time': avg_time,
            'min_time': min_time,
            'max_time': max_time,
            'result_size': result_size,
            'row_count': row_count
        }

# Usage
profiler = QueryProfiler(
    influx_url="http://localhost:8086",
    org="my-org",
    token="YOUR_TOKEN"
)

# Profile different queries
queries = {
    'simple': """
        SELECT * FROM cpu_metrics 
        WHERE time >= now() - INTERVAL '1 hour' 
        LIMIT 1000
    """,
    'aggregation': """
        SELECT 
            date_bin(INTERVAL '5 minutes', time) as bucket,
            host,
            AVG(cpu_percent) as avg_cpu
        FROM cpu_metrics
        WHERE time >= now() - INTERVAL '24 hours'
        GROUP BY bucket, host
    """,
    'complex': """
        SELECT 
            host,
            AVG(cpu_percent) as avg_cpu,
            MAX(cpu_percent) as max_cpu,
            APPROX_PERCENTILE(cpu_percent, 0.95) as p95_cpu
        FROM cpu_metrics
        WHERE time >= now() - INTERVAL '7 days'
        GROUP BY host
        HAVING AVG(cpu_percent) > 50
        ORDER BY avg_cpu DESC
    """
}

for name, query in queries.items():
    print(f"\n{'='*60}")
    print(f"Query: {name}")
    print('='*60)
    profiler.profile_query(query, runs=5)
```

---

## 8.5 Memory Management

### **Memory Usage Patterns:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            InfluxDB Memory Usage                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  In-Memory Cache (Hot Data)                             â”‚
â”‚  â”œâ”€ Recent writes (last 5-15 minutes)                   â”‚
â”‚  â”œâ”€ Size: 500 MB - 2 GB (configurable)                  â”‚
â”‚  â””â”€ Purpose: Fast read access                           â”‚
â”‚                                                          â”‚
â”‚  Query Execution Memory                                  â”‚
â”‚  â”œâ”€ Arrow buffers                                        â”‚
â”‚  â”œâ”€ Aggregation buffers                                  â”‚
â”‚  â”œâ”€ Sort buffers                                         â”‚
â”‚  â””â”€ Size: Varies by query (100 MB - 4 GB)               â”‚
â”‚                                                          â”‚
â”‚  WAL Buffer                                              â”‚
â”‚  â”œâ”€ Write-ahead log in memory                            â”‚
â”‚  â”œâ”€ Size: 100-500 MB                                     â”‚
â”‚  â””â”€ Flushes to disk periodically                        â”‚
â”‚                                                          â”‚
â”‚  Index Cache                                             â”‚
â”‚  â”œâ”€ Tag index in memory                                  â”‚
â”‚  â”œâ”€ Size: 5-15% of data size                            â”‚
â”‚  â””â”€ Critical for query performance                       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Recommended Memory:
- Small:   4-8 GB   (<1M points/day)
- Medium:  16-32 GB (1-10M points/day)
- Large:   64+ GB   (>10M points/day)
```

---

### **Memory Monitoring Script:**

python

```python
import psutil
import time

class MemoryMonitor:
    def __init__(self, process_name="influxd"):
        self.process_name = process_name
        self.process = self.find_process()
  
    def find_process(self):
        """Find InfluxDB process"""
        for proc in psutil.process_iter(['pid', 'name']):
            if self.process_name in proc.info['name']:
                return psutil.Process(proc.info['pid'])
        return None
  
    def get_memory_stats(self):
        """Get current memory usage"""
        if not self.process:
            return None
  
        mem_info = self.process.memory_info()
        mem_percent = self.process.memory_percent()
  
        return {
            'rss_mb': mem_info.rss / 1024 / 1024,  # Resident Set Size
            'vms_mb': mem_info.vms / 1024 / 1024,  # Virtual Memory Size
            'percent': mem_percent,
            'num_threads': self.process.num_threads()
        }
  
    def monitor_continuous(self, interval=5):
        """Monitor memory continuously"""
        print(f"Monitoring InfluxDB memory usage (interval={interval}s)")
        print(f"{'Time':<20} {'RSS (MB)':<12} {'VMS (MB)':<12} {'%':<8} {'Threads'}")
        print("-" * 70)
  
        while True:
            stats = self.get_memory_stats()
            if stats:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                print(f"{timestamp:<20} "
                      f"{stats['rss_mb']:<12.2f} "
                      f"{stats['vms_mb']:<12.2f} "
                      f"{stats['percent']:<8.2f} "
                      f"{stats['num_threads']}")
  
            time.sleep(interval)

# Usage
monitor = MemoryMonitor()
monitor.monitor_continuous(interval=10)
```

---

## 8.6 Disk I/O Optimization

### **SSD vs HDD Performance:**

```
Operation          SSD          HDD         Speedup
-----------------------------------------------------
Random Read        <0.1ms       10-15ms     100-150x
Sequential Read    500 MB/s     150 MB/s    3-4x
Random Write       <0.1ms       10-15ms     100-150x
Sequential Write   400 MB/s     120 MB/s    3-4x
IOPS              50,000+      100-200     250-500x

Recommendation: Always use SSD for InfluxDB!
```

---

### **Disk I/O Monitoring:**

bash

```bash
#!/bin/bash
# disk_io_monitor.sh

echo "Monitoring InfluxDB disk I/O..."
echo "Press Ctrl+C to stop"
echo ""

while true; do
    echo "$(date '+%Y-%m-%d %H:%M:%S')"
  
    # Disk usage
    df -h /var/lib/influxdb | tail -1
  
    # I/O stats (Linux)
    iostat -x 1 1 | grep -A 1 "Device"
  
    echo "---"
    sleep 5
done
```

---

### **File System Optimization:**

**Recommended File System Settings:**

bash

```bash
# Mount options for InfluxDB data directory
# Add to /etc/fstab:

/dev/sdb1  /var/lib/influxdb  ext4  noatime,nodiratime,discard  0  2

# Explanation:
# - noatime:      Don't update access time (faster)
# - nodiratime:   Don't update directory access time
# - discard:      Enable TRIM for SSD
```

**XFS vs EXT4:**

```
XFS (Recommended):
âœ“ Better performance for large files
âœ“ Better concurrent write handling
âœ“ Better scalability

EXT4 (Also good):
âœ“ More mature
âœ“ Better tooling
âœ“ Slightly better for small files
```

---

## 8.7 Network Optimization

### **Optimization 1: Batching**

python

```python
# Network overhead comparison

# âŒ Bad: Individual requests
for point in data_points:  # 10,000 points
    write_single(point)
    # Network overhead: 10,000 Ã— 5ms = 50 seconds!

# âœ… Good: Batched requests
for batch in chunks(data_points, 5000):  # 2 batches
    write_batch(batch)
    # Network overhead: 2 Ã— 5ms = 10ms
```

---

### **Optimization 2: Compression**

python

```python
# Data size comparison

# Without compression
original_size = 10 MB
transfer_time = 10 MB / 100 Mbps = 0.8 seconds

# With gzip compression (7x)
compressed_size = 1.4 MB
transfer_time = 1.4 MB / 100 Mbps = 0.11 seconds
compression_time = 0.05 seconds
total = 0.16 seconds

# Speedup: 5x faster (0.8s â†’ 0.16s)
```

---

### **Optimization 3: Keep-Alive Connections**

python

```python
# Connection reuse

# âŒ Bad: New connection per request
for i in range(1000):
    requests.post(url, data=data)
    # TCP handshake: 3 Ã— 20ms = 60ms overhead per request
    # Total overhead: 1000 Ã— 60ms = 60 seconds!

# âœ… Good: Reuse connection
session = requests.Session()
for i in range(1000):
    session.post(url, data=data)
    # TCP handshake: 1 Ã— 60ms = 60ms total
    # Total overhead: 60ms (1000x faster!)
```

---

## 8.8 Common Anti-Patterns to Avoid

### **Anti-Pattern 1: High Cardinality Tags**

sql

```sql
-- âŒ BAD: UUID as tag
CREATE measurement user_events WITH TAGS (
    user_id UUID  -- 1,000,000+ unique values!
)

-- Problem:
-- - Huge index size (5+ GB)
-- - Slow queries
-- - High memory usage

-- âœ… GOOD: UUID as field, segment as tag
CREATE measurement user_events WITH TAGS (
    user_segment VARCHAR  -- 'free', 'premium', 'enterprise'
)
WITH FIELDS (
    user_id VARCHAR  -- Store UUID here
)
```

---

### **Anti-Pattern 2: No Time Filter**

sql

```sql
-- âŒ BAD: No time range (scans everything!)
SELECT AVG(cpu_percent)
FROM cpu_metrics
WHERE host = 'server01'

-- Scans: 1 year of data (50+ GB)
-- Time: 30+ seconds

-- âœ… GOOD: Always filter by time
SELECT AVG(cpu_percent)
FROM cpu_metrics
WHERE host = 'server01'
  AND time >= now() - INTERVAL '1 hour'

-- Scans: 1 hour of data (5 MB)
-- Time: 50ms (600x faster!)
```

---

### **Anti-Pattern 3: SELECT * on Large Datasets**

sql

```sql
-- âŒ BAD: Select everything
SELECT *
FROM system_metrics
WHERE time >= now() - INTERVAL '7 days'

-- Returns: 1,000,000 rows Ã— 20 columns = 200 MB
-- Time: 5+ seconds

-- âœ… GOOD: Select only needed columns
SELECT time, host, cpu_percent
FROM system_metrics
WHERE time >= now() - INTERVAL '7 days'

-- Returns: 1,000,000 rows Ã— 3 columns = 30 MB
-- Time: 500ms (10x faster!)
```

---

### **Anti-Pattern 4: Client-Side Aggregation**

python

```python
# âŒ BAD: Fetch all data and aggregate in code
results = query("SELECT value FROM metrics WHERE time >= now() - INTERVAL '30 days'")
average = sum(r['value'] for r in results) / len(results)

# Transfer: 10,000,000 rows
# Time: 60+ seconds

# âœ… GOOD: Server-side aggregation
result = query("SELECT AVG(value) as avg FROM metrics WHERE time >= now() - INTERVAL '30 days'")
average = result[0]['avg']

# Transfer: 1 row
# Time: 2 seconds (30x faster!)
```

---

### **Anti-Pattern 5: Single Point Writes**

python

```python
# âŒ BAD: Write one point at a time
for point in data_points:  # 100,000 points
    influx.write(bucket, point)
    # Time: 100,000 Ã— 10ms = 1000 seconds (16 minutes!)

# âœ… GOOD: Batch writes
for batch in chunks(data_points, 5000):  # 20 batches
    influx.write(bucket, batch)
    # Time: 20 Ã— 100ms = 2 seconds (500x faster!)
```

---

## 8.9 Performance Tuning Checklist

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         InfluxDB Performance Checklist                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Schema Design:                                          â”‚
â”‚  â˜ Tags have low cardinality (<10K unique values)       â”‚
â”‚  â˜ High-cardinality data stored as fields               â”‚
â”‚  â˜ Appropriate tag vs field selection                   â”‚
â”‚  â˜ Consistent naming conventions                        â”‚
â”‚                                                          â”‚
â”‚  Write Performance:                                      â”‚
â”‚  â˜ Batch size: 5K-10K points                            â”‚
â”‚  â˜ Connection pooling enabled                           â”‚
â”‚  â˜ Parallel writes (4-8 workers)                        â”‚
â”‚  â˜ Compression enabled for large batches                â”‚
â”‚  â˜ Retry logic with exponential backoff                 â”‚
â”‚                                                          â”‚
â”‚  Query Performance:                                      â”‚
â”‚  â˜ Always filter by time range                          â”‚
â”‚  â˜ Use indexed tags for filtering                       â”‚
â”‚  â˜ Select only needed columns                           â”‚
â”‚  â˜ Aggregate at server, not client                      â”‚
â”‚  â˜ Appropriate time windows for aggregation             â”‚
â”‚  â˜ Use LIMIT for large result sets                      â”‚
â”‚                                                          â”‚
â”‚  Infrastructure:                                         â”‚
â”‚  â˜ SSD storage (not HDD)                                â”‚
â”‚  â˜ Sufficient RAM (16+ GB for production)               â”‚
â”‚  â˜ Multi-core CPU (4+ cores)                            â”‚
â”‚  â˜ Fast network (1+ Gbps)                               â”‚
â”‚  â˜ Proper file system (XFS/EXT4 with noatime)           â”‚
â”‚                                                          â”‚
â”‚  Maintenance:                                            â”‚
â”‚  â˜ Retention policies configured                        â”‚
â”‚  â˜ Downsampling for old data                            â”‚
â”‚  â˜ Regular backups                                      â”‚
â”‚  â˜ Monitoring and alerting                              â”‚
â”‚  â˜ Regular compaction                                   â”‚
â”‚                                                          â”‚
â”‚  Monitoring:                                             â”‚
â”‚  â˜ Track write throughput                               â”‚
â”‚  â˜ Monitor query latency (p50, p95, p99)                â”‚
â”‚  â˜ Watch memory usage                                   â”‚
â”‚  â˜ Monitor disk I/O                                     â”‚
â”‚  â˜ Track series cardinality                             â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8.10 Capacity Planning

### **Estimation Formula:**

python

```python
def estimate_storage(
    points_per_second,
    point_size_bytes,
    retention_days,
    compression_ratio=5
):
    """
    Estimate storage requirements
  
    Args:
        points_per_second: Write rate (points/sec)
        point_size_bytes: Average point size in bytes
        retention_days: How long to keep data
        compression_ratio: Parquet compression (typically 3-10x)
    """
  
    # Calculate daily storage
    points_per_day = points_per_second * 86400
    raw_bytes_per_day = points_per_day * point_size_bytes
  
    # Apply compression
    compressed_bytes_per_day = raw_bytes_per_day / compression_ratio
  
    # Calculate total storage for retention period
    total_storage_gb = (compressed_bytes_per_day * retention_days) / (1024**3)
  
    # Add overhead (WAL, index, etc) - typically 20%
    total_with_overhead_gb = total_storage_gb * 1.2
  
    # Recommended provisioned (2x for safety)
    recommended_gb = total_with_overhead_gb * 2
  
    print(f"Capacity Planning Results:")
    print(f"  Write rate: {points_per_second:,} points/sec")
    print(f"  Points per day: {points_per_day:,}")
    print(f"  Raw data per day: {raw_bytes_per_day/1024**3:.2f} GB")
    print(f"  Compressed per day: {compressed_bytes_per_day/1024**3:.2f} GB")
    print(f"  Retention period: {retention_days} days")
    print(f"  Total storage needed: {total_storage_gb:.2f} GB")
    print(f"  With overhead (20%): {total_with_overhead_gb:.2f} GB")
    print(f"  Recommended provision: {recommended_gb:.2f} GB")
  
    return recommended_gb

# Example: IoT monitoring system
estimate_storage(
    points_per_second=1000,     # 1000 sensors Ã— 1 point/sec
    point_size_bytes=200,       # ~200 bytes per point
    retention_days=90,          # 90-day retention
    compression_ratio=7         # Parquet compression
)
```

**Output:**

```
Capacity Planning Results:
  Write rate: 1,000 points/sec
  Points per day: 86,400,000
  Raw data per day: 16.13 GB
  Compressed per day: 2.30 GB
  Retention period: 90 days
  Total storage needed: 207.40 GB
  With overhead (20%): 248.88 GB
  Recommended provision: 497.76 GB (~500 GB)
```

---

### **Hardware Sizing Guide:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Hardware Sizing Recommendations                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Small Deployment (10K points/sec):                     â”‚
â”‚  â”œâ”€ CPU: 4 cores                                        â”‚
â”‚  â”œâ”€ RAM: 8-16 GB                                        â”‚
â”‚  â”œâ”€ Disk: 500 GB SSD                                    â”‚
â”‚  â”œâ”€ Network: 1 Gbps                                     â”‚
â”‚  â””â”€ Use case: Single application monitoring             â”‚
â”‚                                                          â”‚
â”‚  Medium Deployment (50K points/sec):                    â”‚
â”‚  â”œâ”€ CPU: 8-16 cores                                     â”‚
â”‚  â”œâ”€ RAM: 32-64 GB                                       â”‚
â”‚  â”œâ”€ Disk: 2 TB NVMe SSD                                â”‚
â”‚  â”œâ”€ Network: 10 Gbps                                    â”‚
â”‚  â””â”€ Use case: Multiple apps, IoT fleet                  â”‚
â”‚                                                          â”‚
â”‚  Large Deployment (200K+ points/sec):                   â”‚
â”‚  â”œâ”€ CPU: 32+ cores                                      â”‚
â”‚  â”œâ”€ RAM: 128+ GB                                        â”‚
â”‚  â”œâ”€ Disk: 10+ TB NVMe SSD (RAID 10)                    â”‚
â”‚  â”œâ”€ Network: 10-40 Gbps                                 â”‚
â”‚  â””â”€ Use case: Enterprise monitoring, time-series DB     â”‚
â”‚                                                          â”‚
â”‚  Clustered Deployment (500K+ points/sec):               â”‚
â”‚  â”œâ”€ Nodes: 3-10 instances                               â”‚
â”‚  â”œâ”€ CPU per node: 16-32 cores                           â”‚
â”‚  â”œâ”€ RAM per node: 64-128 GB                             â”‚
â”‚  â”œâ”€ Disk per node: 5 TB NVMe SSD                        â”‚
â”‚  â”œâ”€ Network: 25-100 Gbps                                â”‚
â”‚  â”œâ”€ Load balancer: HAProxy/Nginx                        â”‚
â”‚  â””â”€ Use case: Massive scale, multi-tenant               â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Takeaways (Module 8)

```
âœ“ Batch writes (5K-10K) = 10-100x faster
âœ“ Always filter by time = 100-1000x faster queries
âœ“ Use indexed tags for filtering
âœ“ Aggregate at server, not client
âœ“ SSD is mandatory for good performance
âœ“ Connection pooling essential for high throughput
âœ“ Compression saves bandwidth (7x reduction)
âœ“ Monitor memory, CPU, disk I/O continuously
âœ“ Capacity planning prevents future issues
âœ“ Avoid high-cardinality tags
```

---

## ğŸ“ Practice Tasks

**Task 1: Write Performance Benchmark**

python

```python
# Create benchmark comparing:
1. Single writes vs batched (100, 1000, 5000, 10000)
2. With/without compression
3. Sequential vs parallel (1, 2, 4, 8 threads)
4. Measure throughput and latency
```

**Task 2: Query Optimization**

sql

```sql
-- Take a slow query and optimize:
1. Add time filter
2. Remove unnecessary columns
3. Push aggregation to server
4. Use appropriate time windows
5. Measure before/after performance
```

**Task 3: Monitoring Dashboard**

python

```python
# Build real-time monitoring:
1. Write throughput (points/sec)
2. Query latency (p50, p95, p99)
3. Memory usage
4. Disk I/O
5. Series cardinality
```

**Task 4: Capacity Planning**

python

```python
# Calculate for your use case:
1. Expected write rate
2. Point size
3. Retention period
4. Storage requirements
5. Hardware specs needed
```

---

## ğŸš€ Next Module Preview

**Module 9: Security & Operations**

* Authentication methods (tokens, OAuth)
* Authorization and permissions
* Token management best practices
* Network security (TLS/SSL)
* Backup strategies
* Monitoring InfluxDB itself
* Alerting and incident response
* Upgrade procedures
* Troubleshooting guide

**Ready for Module 9?** Let me know! ğŸ’ª Ab production security aur operations seekhenge!

yes, Ready for Module 9
